{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc6e2ed",
   "metadata": {},
   "source": [
    "# üìä Chapter 04: scikit-learn - Machine Learning with Multiple Datasets\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this chapter, you will:\n",
    "- ‚úÖ Understand scikit-learn's estimator API and workflow\n",
    "- ‚úÖ Build classification models (predict categories)\n",
    "- ‚úÖ Build regression models (predict numbers)\n",
    "- ‚úÖ Apply ML techniques on **3 different datasets**\n",
    "- ‚úÖ Evaluate model performance with metrics\n",
    "- ‚úÖ Perform cross-validation and hyperparameter tuning\n",
    "\n",
    "---\n",
    "\n",
    "## üìÅ Datasets Used\n",
    "\n",
    "### 1Ô∏è‚É£ Iris Flowers (Classification)\n",
    "**Purpose**: Classify flower species\n",
    "- 150 samples, 4 features\n",
    "- 3 classes (Setosa, Versicolor, Virginica)\n",
    "\n",
    "### 2Ô∏è‚É£ Wine Quality (Classification)\n",
    "**Purpose**: Classify wine types\n",
    "- 178 samples, 13 features\n",
    "- 3 classes of wine\n",
    "\n",
    "### 3Ô∏è‚É£ California Housing (Regression)\n",
    "**Purpose**: Predict house prices\n",
    "- 20,640 samples, 8 features\n",
    "- Continuous target (price)\n",
    "\n",
    "**Why 3 datasets?** Practice both classification AND regression!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Table of Contents\n",
    "1. [Introduction to scikit-learn](#intro)\n",
    "2. [Loading Datasets](#loading)\n",
    "3. [Train/Test Split](#split)\n",
    "4. [Data Preprocessing](#preprocessing)\n",
    "5. [Classification Models](#classification)\n",
    "6. [Regression Models](#regression)\n",
    "7. [Model Evaluation](#evaluation)\n",
    "8. [Cross-Validation](#cv)\n",
    "9. [Pipelines](#pipelines)\n",
    "10. [Practice Exercises](#exercises)\n",
    "11. [Next Steps: Projects](#projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a613c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core scikit-learn modules\n",
    "# sklearn.datasets - Built-in datasets for practice\n",
    "from sklearn.datasets import load_iris, load_wine, fetch_california_housing\n",
    "\n",
    "# sklearn.model_selection - Train/test split, cross-validation\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# sklearn.preprocessing - Data scaling and transformation\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# sklearn.linear_model - Linear models (Linear Regression, Logistic Regression)\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# sklearn.ensemble - Ensemble methods (Random Forest, Gradient Boosting)\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# sklearn.metrics - Model evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "# Import supporting libraries\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation\n",
    "import matplotlib.pyplot as plt  # For visualization\n",
    "import seaborn as sns  # For statistical plots\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "# This ensures we get same results every time we run the code\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check versions\n",
    "import sklearn\n",
    "print(f\"scikit-learn version: {sklearn.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"pandas version: {pd.__version__}\")\n",
    "print(\"\\n‚úÖ All libraries imported successfully!\")\n",
    "print(\"\\nüéØ Ready to build machine learning models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c7675",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"intro\"></a>\n",
    "## **1. Introduction to scikit-learn**\n",
    "\n",
    "**What is scikit-learn?**\n",
    "- **The** machine learning library for Python (since 2007)\n",
    "- Built on NumPy, SciPy, and matplotlib\n",
    "- Provides simple, efficient tools for data mining and analysis\n",
    "- Open-source and commercially usable (BSD license)\n",
    "\n",
    "**Why scikit-learn?**\n",
    "- ‚úÖ **Consistent API** - All models use same `.fit()`, `.predict()` pattern\n",
    "- ‚úÖ **Comprehensive** - Classification, regression, clustering, dimensionality reduction\n",
    "- ‚úÖ **Well-documented** - Excellent tutorials and examples\n",
    "- ‚úÖ **Production-ready** - Used in industry worldwide\n",
    "\n",
    "**The scikit-learn Workflow:**\n",
    "```\n",
    "1. Load Data ‚Üí 2. Split (train/test) ‚Üí 3. Preprocess (scale, encode)\n",
    "   ‚Üì\n",
    "4. Choose Model ‚Üí 5. Train (.fit) ‚Üí 6. Predict ‚Üí 7. Evaluate\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Features (X)**: Input variables (sepal length, petal width, etc.)\n",
    "- **Target (y)**: Output variable we want to predict (species, price)\n",
    "- **Training**: Model learns patterns from training data\n",
    "- **Testing**: Model evaluated on unseen test data\n",
    "- **Overfitting**: Model memorizes training data, fails on new data\n",
    "- **Underfitting**: Model too simple, can't capture patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16cf12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"loading\"></a>\n",
    "## **2. Loading 3 Machine Learning Datasets**\n",
    "\n",
    "We'll practice on 3 classic datasets:\n",
    "1. **Iris** - Simple 3-class classification (flowers)\n",
    "2. **Wine** - Multi-feature classification (wine types)\n",
    "3. **California Housing** - Regression (predicting house prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ad1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"LOADING 3 MACHINE LEARNING DATASETS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET 1: Iris Flowers - Classification\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ DATASET 1: Iris Flowers (Classification)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Load iris dataset\n",
    "# Returns a Bunch object (dictionary-like structure)\n",
    "iris = load_iris()\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X_iris = iris.data  # Feature matrix: shape (150, 4)\n",
    "y_iris = iris.target  # Target vector: shape (150,)\n",
    "\n",
    "# Convert to DataFrame for easier viewing\n",
    "iris_df = pd.DataFrame(X_iris, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target_names[y_iris]  # Map 0,1,2 to species names\n",
    "\n",
    "print(f\"Shape: {X_iris.shape}\")  # (samples, features)\n",
    "print(f\"Features: {iris.feature_names}\")\n",
    "print(f\"Target classes: {iris.target_names}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(pd.Series(y_iris).value_counts().sort_index())\n",
    "print(\"\\nFirst 3 samples:\")\n",
    "print(iris_df.head(3))\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET 2: Wine - Classification  \n",
    "# ============================================================================\n",
    "print(\"\\n\\n2Ô∏è‚É£ DATASET 2: Wine Quality (Classification)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Load wine dataset\n",
    "wine = load_wine()\n",
    "\n",
    "X_wine = wine.data  # Features: shape (178, 13)\n",
    "y_wine = wine.target  # Target: shape (178,)\n",
    "\n",
    "wine_df = pd.DataFrame(X_wine, columns=wine.feature_names)\n",
    "wine_df['wine_class'] = wine.target_names[y_wine]\n",
    "\n",
    "print(f\"Shape: {X_wine.shape}\")\n",
    "print(f\"Number of features: {X_wine.shape[1]}\")\n",
    "print(f\"Target classes: {wine.target_names}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(pd.Series(y_wine).value_counts().sort_index())\n",
    "print(\"\\nFirst 3 features:\")\n",
    "print(wine.feature_names[:3])\n",
    "print(wine_df.head(3))\n",
    "\n",
    "# ============================================================================\n",
    "# DATASET 3: California Housing - Regression\n",
    "# ============================================================================\n",
    "print(\"\\n\\n3Ô∏è‚É£ DATASET 3: California Housing (Regression)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Load housing dataset\n",
    "# This downloads data from internet on first run\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_housing = housing.data  # Features: shape (20640, 8)\n",
    "y_housing = housing.target  # Target (house prices in $100,000s)\n",
    "\n",
    "housing_df = pd.DataFrame(X_housing, columns=housing.feature_names)\n",
    "housing_df['price'] = y_housing\n",
    "\n",
    "print(f\"Shape: {X_housing.shape}\")\n",
    "print(f\"Features: {housing.feature_names}\")\n",
    "print(f\"Target: House price (in $100,000s)\")\n",
    "print(f\"Price range: ${y_housing.min():.2f} - ${y_housing.max():.2f} (hundreds of thousands)\")\n",
    "print(f\"Mean price: ${y_housing.mean():.2f} (= ${y_housing.mean()*100000:.0f})\")\n",
    "print(\"\\nFirst 3 samples:\")\n",
    "print(housing_df.head(3))\n",
    "\n",
    "print(\"\\n‚úÖ All 3 datasets loaded successfully!\")\n",
    "print(f\"Total samples: {len(X_iris) + len(X_wine) + len(X_housing):,}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e49cef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"split\"></a>\n",
    "## **3. Train/Test Split - The Foundation**\n",
    "\n",
    "**Why Split Data?**\n",
    "- **Training set**: Model learns patterns from this data\n",
    "- **Test set**: Evaluate model on unseen data (simulates real-world performance)\n",
    "- **Rule of thumb**: 70-80% train, 20-30% test\n",
    "\n",
    "**Key Parameters:**\n",
    "- `test_size=0.2` - Use 20% for testing\n",
    "- `random_state=42` - Makes split reproducible\n",
    "- `stratify=y` - Keeps class balance in splits\n",
    "\n",
    "**Critical Rule:** NEVER let model see test data during training!\n",
    "\n",
    "---\n",
    "\n",
    "<a id=\"preprocessing\"></a>\n",
    "## **4. Data Preprocessing - Preparing for ML**\n",
    "\n",
    "**Why Preprocess?**\n",
    "- Different features have different scales (age: 0-100, income: 0-1000000)\n",
    "- ML algorithms perform better with normalized data\n",
    "- Some algorithms (KNN, SVM) require scaling\n",
    "\n",
    "**Common Preprocessing:**\n",
    "1. **StandardScaler**: Mean=0, StdDev=1 (most common)\n",
    "2. **MinMaxScaler**: Scale to range [0, 1]\n",
    "3. **Label Encoding**: Convert categories to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca4a82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAIN/TEST SPLIT & PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: Train/Test Split for All 3 Datasets\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ SPLITTING DATA INTO TRAIN AND TEST SETS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Split Iris dataset\n",
    "# train_test_split(X, y, test_size, random_state, stratify)\n",
    "# stratify=y ensures each split has same class proportions\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
    "    X_iris, y_iris,\n",
    "    test_size=0.2,      # 20% for testing, 80% for training\n",
    "    random_state=42,    # For reproducibility\n",
    "    stratify=y_iris     # Keep class balance\n",
    ")\n",
    "\n",
    "print(\"üå∏ IRIS:\")\n",
    "print(f\"  Total samples: {len(X_iris)}\")\n",
    "print(f\"  Training set: {len(X_train_iris)} samples ({len(X_train_iris)/len(X_iris)*100:.0f}%)\")\n",
    "print(f\"  Test set: {len(X_test_iris)} samples ({len(X_test_iris)/len(X_iris)*100:.0f}%)\")\n",
    "print(f\"  Features shape: {X_train_iris.shape}\")\n",
    "print(f\"  Train class distribution: {np.bincount(y_train_iris)}\")\n",
    "print(f\"  Test class distribution: {np.bincount(y_test_iris)}\")\n",
    "\n",
    "# Split Wine dataset\n",
    "X_train_wine, X_test_wine, y_train_wine, y_test_wine = train_test_split(\n",
    "    X_wine, y_wine,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_wine\n",
    ")\n",
    "\n",
    "print(\"\\nüç∑ WINE:\")\n",
    "print(f\"  Total samples: {len(X_wine)}\")\n",
    "print(f\"  Training set: {len(X_train_wine)} samples\")\n",
    "print(f\"  Test set: {len(X_test_wine)} samples\")\n",
    "print(f\"  Train class distribution: {np.bincount(y_train_wine)}\")\n",
    "\n",
    "# Split Housing dataset (no stratify for regression)\n",
    "X_train_housing, X_test_housing, y_train_housing, y_test_housing = train_test_split(\n",
    "    X_housing, y_housing,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    "    # No stratify for regression (only for classification)\n",
    ")\n",
    "\n",
    "print(\"\\nüè† HOUSING:\")\n",
    "print(f\"  Total samples: {len(X_housing):,}\")\n",
    "print(f\"  Training set: {len(X_train_housing):,} samples\")\n",
    "print(f\"  Test set: {len(X_test_housing):,} samples\")\n",
    "print(f\"  Train price mean: ${y_train_housing.mean():.2f} (hundreds of thousands)\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: Feature Scaling - StandardScaler\n",
    "# ============================================================================\n",
    "print(\"\\n\\n2Ô∏è‚É£ FEATURE SCALING WITH STANDARDSCALER\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Demonstrate need for scaling\n",
    "print(\"BEFORE SCALING - Iris feature ranges:\")\n",
    "print(f\"  Sepal length: {X_train_iris[:, 0].min():.2f} to {X_train_iris[:, 0].max():.2f}\")\n",
    "print(f\"  Sepal width:  {X_train_iris[:, 1].min():.2f} to {X_train_iris[:, 1].max():.2f}\")\n",
    "print(f\"  Petal length: {X_train_iris[:, 2].min():.2f} to {X_train_iris[:, 2].max():.2f}\")\n",
    "print(f\"  Petal width:  {X_train_iris[:, 3].min():.2f} to {X_train_iris[:, 3].max():.2f}\")\n",
    "\n",
    "# Create scaler for Iris\n",
    "scaler_iris = StandardScaler()\n",
    "\n",
    "# IMPORTANT: Fit scaler ONLY on training data\n",
    "# This prevents data leakage from test set\n",
    "scaler_iris.fit(X_train_iris)\n",
    "\n",
    "# Transform both train and test using training statistics\n",
    "X_train_iris_scaled = scaler_iris.transform(X_train_iris)\n",
    "X_test_iris_scaled = scaler_iris.transform(X_test_iris)\n",
    "\n",
    "print(\"\\nAFTER SCALING - Iris (mean‚âà0, std‚âà1):\")\n",
    "print(f\"  Feature 0 - Mean: {X_train_iris_scaled[:, 0].mean():.6f}, Std: {X_train_iris_scaled[:, 0].std():.6f}\")\n",
    "print(f\"  Feature 1 - Mean: {X_train_iris_scaled[:, 1].mean():.6f}, Std: {X_train_iris_scaled[:, 1].std():.6f}\")\n",
    "\n",
    "# Scale Wine dataset\n",
    "scaler_wine = StandardScaler()\n",
    "X_train_wine_scaled = scaler_wine.fit_transform(X_train_wine)  # fit + transform in one step\n",
    "X_test_wine_scaled = scaler_wine.transform(X_test_wine)\n",
    "\n",
    "print(\"\\nüç∑ WINE scaled:\")\n",
    "print(f\"  Original feature 0 range: {X_train_wine[:, 0].min():.2f} to {X_train_wine[:, 0].max():.2f}\")\n",
    "print(f\"  Scaled feature 0 range: {X_train_wine_scaled[:, 0].min():.2f} to {X_train_wine_scaled[:, 0].max():.2f}\")\n",
    "\n",
    "# Scale Housing dataset\n",
    "scaler_housing = StandardScaler()\n",
    "X_train_housing_scaled = scaler_housing.fit_transform(X_train_housing)\n",
    "X_test_housing_scaled = scaler_housing.transform(X_test_housing)\n",
    "\n",
    "print(\"\\nüè† HOUSING scaled:\")\n",
    "print(f\"  Before: Feature means range from {X_train_housing.mean(axis=0).min():.2f} to {X_train_housing.mean(axis=0).max():.2f}\")\n",
    "print(f\"  After:  All features have mean ‚âà 0, std ‚âà 1\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: Why Scaling Matters - Visualization\n",
    "# ============================================================================\n",
    "print(\"\\n\\n3Ô∏è‚É£ VISUALIZING IMPACT OF SCALING\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Before scaling\n",
    "ax = axes[0]\n",
    "ax.scatter(X_train_iris[:, 2], X_train_iris[:, 3], \n",
    "          c=y_train_iris, cmap='viridis', alpha=0.6, s=50)\n",
    "ax.set_xlabel('Petal Length (cm)', fontsize=11)\n",
    "ax.set_ylabel('Petal Width (cm)', fontsize=11)\n",
    "ax.set_title('BEFORE Scaling - Different Scales', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# After scaling\n",
    "ax = axes[1]\n",
    "ax.scatter(X_train_iris_scaled[:, 2], X_train_iris_scaled[:, 3], \n",
    "          c=y_train_iris, cmap='viridis', alpha=0.6, s=50)\n",
    "ax.set_xlabel('Petal Length (scaled)', fontsize=11)\n",
    "ax.set_ylabel('Petal Width (scaled)', fontsize=11)\n",
    "ax.set_title('AFTER Scaling - Normalized', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Scaling makes features comparable!\")\n",
    "print(\"‚úÖ Most ML algorithms perform better with scaled data\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: Data Leakage Warning\n",
    "# ============================================================================\n",
    "print(\"\\n\\n4Ô∏è‚É£ CRITICAL: AVOIDING DATA LEAKAGE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"‚ùå WRONG WAY (causes data leakage):\")\n",
    "print(\"  scaler.fit(entire_dataset)  # DON'T DO THIS!\")\n",
    "print(\"  X_train_scaled = scaler.transform(X_train)\")\n",
    "print(\"  X_test_scaled = scaler.transform(X_test)\")\n",
    "print(\"\\n  Problem: Test set statistics leaked into training!\")\n",
    "\n",
    "print(\"\\n‚úÖ CORRECT WAY:\")\n",
    "print(\"  scaler.fit(X_train)  # Fit ONLY on training data\")\n",
    "print(\"  X_train_scaled = scaler.transform(X_train)\")\n",
    "print(\"  X_test_scaled = scaler.transform(X_test)  # Use training statistics\")\n",
    "print(\"\\n  Result: Test set remains truly unseen!\")\n",
    "\n",
    "print(\"\\n‚úÖ Train/test split and preprocessing completed for all 3 datasets!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d20033",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **5. Classification Models - Predicting Categories**\n",
    "\n",
    "**What is Classification?**\n",
    "- Predicting which category (class) something belongs to\n",
    "- Output: Discrete labels (0, 1, 2 or \"setosa\", \"versicolor\", \"virginica\")\n",
    "- Examples: Email spam detection, disease diagnosis, species identification\n",
    "\n",
    "**Models We'll Use:**\n",
    "1. **Logistic Regression** - Simple, interpretable, great baseline\n",
    "2. **Random Forest** - Ensemble of decision trees, handles non-linear patterns\n",
    "\n",
    "**Datasets:**\n",
    "- üå∏ **Iris**: 3 flower species based on 4 measurements\n",
    "- üç∑ **Wine**: 3 wine types based on 13 chemical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf277606",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CLASSIFICATION MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: Logistic Regression on Iris Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ LOGISTIC REGRESSION ON IRIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create logistic regression model\n",
    "# LogisticRegression is a LINEAR classifier despite the name\n",
    "# It works well for linearly separable classes\n",
    "lr_iris = LogisticRegression(\n",
    "    max_iter=200,      # Maximum iterations for convergence\n",
    "    random_state=42    # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "# .fit() learns the decision boundaries from training data\n",
    "print(\"Training Logistic Regression on Iris (120 samples)...\")\n",
    "lr_iris.fit(X_train_iris_scaled, y_train_iris)\n",
    "\n",
    "# Make predictions on test set\n",
    "# .predict() returns predicted class labels\n",
    "y_pred_iris_lr = lr_iris.predict(X_test_iris_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "# accuracy = (correct predictions) / (total predictions)\n",
    "accuracy_iris_lr = accuracy_score(y_test_iris, y_pred_iris_lr)\n",
    "\n",
    "print(f\"\\n‚úÖ Model trained successfully!\")\n",
    "print(f\"üìä Test Accuracy: {accuracy_iris_lr:.2%}\")\n",
    "print(f\"   Got {int(accuracy_iris_lr * len(y_test_iris))} out of {len(y_test_iris)} correct\")\n",
    "\n",
    "# Show some predictions vs actual\n",
    "print(\"\\nüîç Sample Predictions (first 10):\")\n",
    "print(f\"   Predicted: {y_pred_iris_lr[:10]}\")\n",
    "print(f\"   Actual:    {y_test_iris[:10]}\")\n",
    "print(f\"   Match:     {y_pred_iris_lr[:10] == y_test_iris[:10]}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "# Shows where model gets confused between classes\n",
    "cm_iris_lr = confusion_matrix(y_test_iris, y_pred_iris_lr)\n",
    "print(\"\\nüìã Confusion Matrix:\")\n",
    "print(\"   Rows = Actual, Columns = Predicted\")\n",
    "print(cm_iris_lr)\n",
    "\n",
    "# Classification Report\n",
    "# Precision, Recall, F1-score for each class\n",
    "print(\"\\nüìà Classification Report:\")\n",
    "print(classification_report(y_test_iris, y_pred_iris_lr, \n",
    "                          target_names=['Setosa', 'Versicolor', 'Virginica']))\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: Random Forest on Iris Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2Ô∏è‚É£ RANDOM FOREST ON IRIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create Random Forest classifier\n",
    "# Ensemble of 100 decision trees voting together\n",
    "rf_iris = RandomForestClassifier(\n",
    "    n_estimators=100,   # Number of trees in the forest\n",
    "    max_depth=5,        # Maximum depth of each tree\n",
    "    random_state=42     # For reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Random Forest (100 trees) on Iris...\")\n",
    "rf_iris.fit(X_train_iris_scaled, y_train_iris)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_iris_rf = rf_iris.predict(X_test_iris_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_iris_rf = accuracy_score(y_test_iris, y_pred_iris_rf)\n",
    "\n",
    "print(f\"\\n‚úÖ Model trained successfully!\")\n",
    "print(f\"üìä Test Accuracy: {accuracy_iris_rf:.2%}\")\n",
    "print(f\"   Improvement over Logistic Regression: {(accuracy_iris_rf - accuracy_iris_lr):.2%}\")\n",
    "\n",
    "# Feature Importance\n",
    "# Which features matter most for prediction?\n",
    "feature_importance_iris = rf_iris.feature_importances_\n",
    "feature_names = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n",
    "\n",
    "print(\"\\nüéØ Feature Importance (which features matter most?):\")\n",
    "for name, importance in zip(feature_names, feature_importance_iris):\n",
    "    print(f\"   {name:15s}: {'‚ñà' * int(importance * 50)} {importance:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: Logistic Regression on Wine Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3Ô∏è‚É£ LOGISTIC REGRESSION ON WINE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Train model on wine dataset\n",
    "lr_wine = LogisticRegression(max_iter=1000, random_state=42)\n",
    "print(\"Training Logistic Regression on Wine (142 samples)...\")\n",
    "lr_wine.fit(X_train_wine_scaled, y_train_wine)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_wine_lr = lr_wine.predict(X_test_wine_scaled)\n",
    "accuracy_wine_lr = accuracy_score(y_test_wine, y_pred_wine_lr)\n",
    "\n",
    "print(f\"\\n‚úÖ Test Accuracy: {accuracy_wine_lr:.2%}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm_wine_lr = confusion_matrix(y_test_wine, y_pred_wine_lr)\n",
    "print(\"\\nüìã Confusion Matrix:\")\n",
    "print(cm_wine_lr)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: Random Forest on Wine Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4Ô∏è‚É£ RANDOM FOREST ON WINE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Train Random Forest\n",
    "rf_wine = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "print(\"Training Random Forest on Wine...\")\n",
    "rf_wine.fit(X_train_wine_scaled, y_train_wine)\n",
    "\n",
    "# Predictions and evaluation\n",
    "y_pred_wine_rf = rf_wine.predict(X_test_wine_scaled)\n",
    "accuracy_wine_rf = accuracy_score(y_test_wine, y_pred_wine_rf)\n",
    "\n",
    "print(f\"\\n‚úÖ Test Accuracy: {accuracy_wine_rf:.2%}\")\n",
    "\n",
    "# Top 5 most important features\n",
    "feature_importance_wine = rf_wine.feature_importances_\n",
    "top_5_indices = np.argsort(feature_importance_wine)[-5:][::-1]\n",
    "\n",
    "print(\"\\nüéØ Top 5 Most Important Features:\")\n",
    "for i, idx in enumerate(top_5_indices, 1):\n",
    "    print(f\"   {i}. Feature {idx}: {feature_importance_wine[idx]:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: Model Comparison Visualization\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"5Ô∏è‚É£ COMPARING ALL CLASSIFICATION MODELS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create comparison bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Iris comparison\n",
    "ax = axes[0]\n",
    "models = ['Logistic\\nRegression', 'Random\\nForest']\n",
    "accuracies = [accuracy_iris_lr, accuracy_iris_rf]\n",
    "bars = ax.bar(models, accuracies, color=['#3498db', '#2ecc71'], alpha=0.7, edgecolor='black')\n",
    "ax.set_ylim(0.9, 1.0)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('üå∏ Iris Classification', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "            f'{height:.2%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Wine comparison\n",
    "ax = axes[1]\n",
    "accuracies = [accuracy_wine_lr, accuracy_wine_rf]\n",
    "bars = ax.bar(models, accuracies, color=['#3498db', '#2ecc71'], alpha=0.7, edgecolor='black')\n",
    "ax.set_ylim(0.9, 1.0)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('üç∑ Wine Classification', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "            f'{height:.2%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Classification models trained and evaluated!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a44fcb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **6. Regression Models - Predicting Numbers**\n",
    "\n",
    "**What is Regression?**\n",
    "- Predicting continuous numerical values\n",
    "- Output: Real numbers (house prices, temperatures, stock prices)\n",
    "- Examples: Predicting house prices, sales forecasting, temperature prediction\n",
    "\n",
    "**Models We'll Use:**\n",
    "1. **Linear Regression** - Assumes linear relationship between features and target\n",
    "2. **Random Forest Regressor** - Handles non-linear patterns, more flexible\n",
    "\n",
    "**Dataset:**\n",
    "- üè† **California Housing**: Predict median house prices based on 8 features\n",
    "  - Features: population, median income, house age, average rooms, etc.\n",
    "  - Target: Median house value (in $100,000s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecca376",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"REGRESSION MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: Linear Regression on Housing Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ LINEAR REGRESSION ON CALIFORNIA HOUSING\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create Linear Regression model\n",
    "# Fits a linear equation: y = w1*x1 + w2*x2 + ... + b\n",
    "lr_housing = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "print(f\"Training Linear Regression on {len(X_train_housing):,} houses...\")\n",
    "lr_housing.fit(X_train_housing_scaled, y_train_housing)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_housing_lr = lr_housing.predict(X_test_housing_scaled)\n",
    "\n",
    "# Calculate regression metrics\n",
    "# MAE = Mean Absolute Error (average difference)\n",
    "mae_lr = mean_absolute_error(y_test_housing, y_pred_housing_lr)\n",
    "# MSE = Mean Squared Error (penalizes large errors more)\n",
    "mse_lr = mean_squared_error(y_test_housing, y_pred_housing_lr)\n",
    "# RMSE = Root Mean Squared Error (same units as target)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "# R¬≤ = Coefficient of determination (1.0 = perfect, 0.0 = bad)\n",
    "r2_lr = r2_score(y_test_housing, y_pred_housing_lr)\n",
    "\n",
    "print(f\"\\n‚úÖ Model trained successfully!\")\n",
    "print(f\"\\nüìä Regression Metrics:\")\n",
    "print(f\"   MAE (Mean Absolute Error):  ${mae_lr:.2f} (hundred thousands)\")\n",
    "print(f\"   MSE (Mean Squared Error):   {mse_lr:.2f}\")\n",
    "print(f\"   RMSE (Root MSE):            ${rmse_lr:.2f}\")\n",
    "print(f\"   R¬≤ Score:                   {r2_lr:.4f} (closer to 1.0 is better)\")\n",
    "print(f\"\\n   üí° On average, predictions are off by ${mae_lr:.2f} √ó $100k = ${mae_lr*100:.0f}k\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nüîç Sample Predictions (first 5 houses):\")\n",
    "for i in range(5):\n",
    "    print(f\"   House {i+1}: Predicted ${y_pred_housing_lr[i]:.2f}, Actual ${y_test_housing.iloc[i]:.2f}, \"\n",
    "          f\"Diff ${abs(y_pred_housing_lr[i] - y_test_housing.iloc[i]):.2f}\")\n",
    "\n",
    "# Model coefficients\n",
    "# Each coefficient shows feature importance (how much it affects price)\n",
    "print(\"\\nüéØ Feature Coefficients (impact on price):\")\n",
    "housing_feature_names = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', \n",
    "                         'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "coefficients = lr_housing.coef_\n",
    "for name, coef in sorted(zip(housing_feature_names, coefficients), \n",
    "                         key=lambda x: abs(x[1]), reverse=True)[:5]:\n",
    "    sign = \"+\" if coef > 0 else \"\"\n",
    "    print(f\"   {name:12s}: {sign}{coef:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: Random Forest Regressor on Housing Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2Ô∏è‚É£ RANDOM FOREST REGRESSOR ON HOUSING\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create Random Forest Regressor\n",
    "# Ensemble of decision trees for regression\n",
    "rf_housing = RandomForestRegressor(\n",
    "    n_estimators=100,   # 100 trees\n",
    "    max_depth=20,       # Maximum depth\n",
    "    random_state=42,\n",
    "    n_jobs=-1           # Use all CPU cores for faster training\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(f\"Training Random Forest (100 trees) on {len(X_train_housing):,} houses...\")\n",
    "rf_housing.fit(X_train_housing_scaled, y_train_housing)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_housing_rf = rf_housing.predict(X_test_housing_scaled)\n",
    "\n",
    "# Calculate metrics\n",
    "mae_rf = mean_absolute_error(y_test_housing, y_pred_housing_rf)\n",
    "mse_rf = mean_squared_error(y_test_housing, y_pred_housing_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test_housing, y_pred_housing_rf)\n",
    "\n",
    "print(f\"\\n‚úÖ Model trained successfully!\")\n",
    "print(f\"\\nüìä Regression Metrics:\")\n",
    "print(f\"   MAE:  ${mae_rf:.2f}\")\n",
    "print(f\"   RMSE: ${rmse_rf:.2f}\")\n",
    "print(f\"   R¬≤:   {r2_rf:.4f}\")\n",
    "print(f\"\\n   ‚ú® Improvement over Linear Regression:\")\n",
    "print(f\"      MAE improved by:  ${mae_lr - mae_rf:.2f}\")\n",
    "print(f\"      R¬≤ improved by:   {r2_rf - r2_lr:.4f}\")\n",
    "\n",
    "# Feature importance for Random Forest\n",
    "feature_importance_housing = rf_housing.feature_importances_\n",
    "\n",
    "print(\"\\nüéØ Feature Importance (what matters most for price?):\")\n",
    "importance_pairs = sorted(zip(housing_feature_names, feature_importance_housing), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "for name, importance in importance_pairs:\n",
    "    bar = '‚ñà' * int(importance * 100)\n",
    "    print(f\"   {name:12s}: {bar} {importance:.3f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: Predictions vs Actual Visualization\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3Ô∏è‚É£ VISUALIZING PREDICTIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Linear Regression: Predicted vs Actual\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(y_test_housing, y_pred_housing_lr, alpha=0.3, s=20, color='blue')\n",
    "ax.plot([y_test_housing.min(), y_test_housing.max()], \n",
    "        [y_test_housing.min(), y_test_housing.max()], \n",
    "        'r--', lw=2, label='Perfect Prediction')\n",
    "ax.set_xlabel('Actual Price ($100k)', fontsize=11)\n",
    "ax.set_ylabel('Predicted Price ($100k)', fontsize=11)\n",
    "ax.set_title('Linear Regression: Predicted vs Actual', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest: Predicted vs Actual\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(y_test_housing, y_pred_housing_rf, alpha=0.3, s=20, color='green')\n",
    "ax.plot([y_test_housing.min(), y_test_housing.max()], \n",
    "        [y_test_housing.min(), y_test_housing.max()], \n",
    "        'r--', lw=2, label='Perfect Prediction')\n",
    "ax.set_xlabel('Actual Price ($100k)', fontsize=11)\n",
    "ax.set_ylabel('Predicted Price ($100k)', fontsize=11)\n",
    "ax.set_title('Random Forest: Predicted vs Actual', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Linear Regression: Residuals (errors)\n",
    "ax = axes[1, 0]\n",
    "residuals_lr = y_test_housing - y_pred_housing_lr\n",
    "ax.scatter(y_pred_housing_lr, residuals_lr, alpha=0.3, s=20, color='blue')\n",
    "ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax.set_xlabel('Predicted Price ($100k)', fontsize=11)\n",
    "ax.set_ylabel('Residual (Actual - Predicted)', fontsize=11)\n",
    "ax.set_title('Linear Regression: Residual Plot', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest: Residuals\n",
    "ax = axes[1, 1]\n",
    "residuals_rf = y_test_housing - y_pred_housing_rf\n",
    "ax.scatter(y_pred_housing_rf, residuals_rf, alpha=0.3, s=20, color='green')\n",
    "ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax.set_xlabel('Predicted Price ($100k)', fontsize=11)\n",
    "ax.set_ylabel('Residual (Actual - Predicted)', fontsize=11)\n",
    "ax.set_title('Random Forest: Residual Plot', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: Model Comparison\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4Ô∏è‚É£ REGRESSION MODEL COMPARISON\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create comparison chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# MAE comparison\n",
    "ax = axes[0]\n",
    "models = ['Linear\\nRegression', 'Random\\nForest']\n",
    "mae_values = [mae_lr, mae_rf]\n",
    "bars = ax.bar(models, mae_values, color=['#3498db', '#2ecc71'], alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('MAE (Mean Absolute Error)', fontsize=12)\n",
    "ax.set_title('Lower is Better', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'${height:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# R¬≤ comparison\n",
    "ax = axes[1]\n",
    "r2_values = [r2_lr, r2_rf]\n",
    "bars = ax.bar(models, r2_values, color=['#3498db', '#2ecc71'], alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('R¬≤ Score', fontsize=12)\n",
    "ax.set_title('Higher is Better (max 1.0)', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            f'{height:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Regression models trained and evaluated!\")\n",
    "print(f\"üèÜ Winner: Random Forest (R¬≤={r2_rf:.4f}, MAE=${mae_rf:.2f})\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c333d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **7. Cross-Validation - Robust Model Evaluation**\n",
    "\n",
    "**Why Cross-Validation?**\n",
    "- Single train/test split might be lucky or unlucky\n",
    "- Cross-validation tests model on multiple different splits\n",
    "- More reliable estimate of true performance\n",
    "\n",
    "**K-Fold Cross-Validation:**\n",
    "1. Split data into K folds (usually 5 or 10)\n",
    "2. Train on K-1 folds, test on remaining fold\n",
    "3. Repeat K times, each fold gets to be test set once\n",
    "4. Average the K scores for final metric\n",
    "\n",
    "**Benefits:**\n",
    "- Uses all data for both training and testing\n",
    "- Reduces variance in performance estimates\n",
    "- Detects overfitting more reliably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424d691",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CROSS-VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: Cross-Validation on Iris Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ 5-FOLD CROSS-VALIDATION ON IRIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Prepare fresh model (not trained yet)\n",
    "lr_iris_cv = LogisticRegression(max_iter=200, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "# cv=5 means split data into 5 parts\n",
    "# Each part becomes test set once, others are training\n",
    "print(\"Running 5-Fold Cross-Validation...\")\n",
    "cv_scores_iris = cross_val_score(\n",
    "    lr_iris_cv,              # Model to evaluate\n",
    "    X_iris_scaled,           # Full feature set (not split)\n",
    "    y_iris,                  # Full target (not split)\n",
    "    cv=5,                    # 5 folds\n",
    "    scoring='accuracy'       # Metric to compute\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Cross-Validation Scores (5 folds):\")\n",
    "for fold, score in enumerate(cv_scores_iris, 1):\n",
    "    bar = '‚ñà' * int(score * 50)\n",
    "    print(f\"   Fold {fold}: {bar} {score:.4f} ({score:.2%})\")\n",
    "\n",
    "print(f\"\\nüìà Summary Statistics:\")\n",
    "print(f\"   Mean Accuracy:  {cv_scores_iris.mean():.4f} ¬± {cv_scores_iris.std():.4f}\")\n",
    "print(f\"   Best Fold:      {cv_scores_iris.max():.4f}\")\n",
    "print(f\"   Worst Fold:     {cv_scores_iris.min():.4f}\")\n",
    "print(f\"   Range:          {cv_scores_iris.max() - cv_scores_iris.min():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: Cross-Validation on Wine Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2Ô∏è‚É£ 5-FOLD CROSS-VALIDATION ON WINE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Random Forest on Wine\n",
    "rf_wine_cv = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "print(\"Running 5-Fold Cross-Validation on Wine...\")\n",
    "cv_scores_wine = cross_val_score(\n",
    "    rf_wine_cv,\n",
    "    X_wine_scaled,\n",
    "    y_wine,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Cross-Validation Scores:\")\n",
    "for fold, score in enumerate(cv_scores_wine, 1):\n",
    "    print(f\"   Fold {fold}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nüìà Mean Accuracy: {cv_scores_wine.mean():.4f} ¬± {cv_scores_wine.std():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: Cross-Validation on Housing Dataset (Regression)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3Ô∏è‚É£ 5-FOLD CROSS-VALIDATION ON HOUSING\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Random Forest Regressor with cross-validation\n",
    "rf_housing_cv = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "print(\"Running 5-Fold Cross-Validation on Housing...\")\n",
    "# For regression, use negative MSE (sklearn convention)\n",
    "# We negate it to get positive MSE values\n",
    "cv_scores_housing_mse = -cross_val_score(\n",
    "    rf_housing_cv,\n",
    "    X_housing_scaled,\n",
    "    y_housing,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error'  # Returns negative MSE\n",
    ")\n",
    "\n",
    "# Convert to RMSE (Root Mean Squared Error)\n",
    "cv_scores_housing_rmse = np.sqrt(cv_scores_housing_mse)\n",
    "\n",
    "print(f\"\\nüìä Cross-Validation RMSE (5 folds):\")\n",
    "for fold, rmse in enumerate(cv_scores_housing_rmse, 1):\n",
    "    print(f\"   Fold {fold}: ${rmse:.2f}\")\n",
    "\n",
    "print(f\"\\nüìà Mean RMSE: ${cv_scores_housing_rmse.mean():.2f} ¬± ${cv_scores_housing_rmse.std():.2f}\")\n",
    "\n",
    "# Also get R¬≤ scores\n",
    "cv_scores_housing_r2 = cross_val_score(\n",
    "    rf_housing_cv,\n",
    "    X_housing_scaled,\n",
    "    y_housing,\n",
    "    cv=5,\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "print(f\"üìà Mean R¬≤: {cv_scores_housing_r2.mean():.4f} ¬± {cv_scores_housing_r2.std():.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: Comparing Single Split vs Cross-Validation\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4Ô∏è‚É£ SINGLE SPLIT VS CROSS-VALIDATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\\nüîç Why Cross-Validation is Better:\")\n",
    "print(\"\\n   Single Train/Test Split:\")\n",
    "print(f\"      ‚Ä¢ One accuracy score: {accuracy_iris_lr:.4f}\")\n",
    "print(f\"      ‚Ä¢ Could be lucky or unlucky\")\n",
    "print(f\"      ‚Ä¢ No idea if result is stable\")\n",
    "\n",
    "print(\"\\n   5-Fold Cross-Validation:\")\n",
    "print(f\"      ‚Ä¢ Five accuracy scores: {cv_scores_iris}\")\n",
    "print(f\"      ‚Ä¢ Mean: {cv_scores_iris.mean():.4f}, Std: {cv_scores_iris.std():.4f}\")\n",
    "print(f\"      ‚Ä¢ More reliable estimate\")\n",
    "print(f\"      ‚Ä¢ Can see variance across folds\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: Visualization of Cross-Validation Results\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"5Ô∏è‚É£ VISUALIZING CROSS-VALIDATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Iris CV scores\n",
    "ax = axes[0]\n",
    "folds = list(range(1, 6))\n",
    "ax.plot(folds, cv_scores_iris, 'o-', linewidth=2, markersize=8, color='blue', label='Fold Scores')\n",
    "ax.axhline(cv_scores_iris.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {cv_scores_iris.mean():.3f}')\n",
    "ax.fill_between(folds, \n",
    "                cv_scores_iris.mean() - cv_scores_iris.std(),\n",
    "                cv_scores_iris.mean() + cv_scores_iris.std(),\n",
    "                alpha=0.2, color='red', label='¬±1 Std Dev')\n",
    "ax.set_xlabel('Fold Number', fontsize=11)\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('üå∏ Iris: Cross-Validation Scores', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(folds)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Wine CV scores\n",
    "ax = axes[1]\n",
    "ax.plot(folds, cv_scores_wine, 'o-', linewidth=2, markersize=8, color='purple', label='Fold Scores')\n",
    "ax.axhline(cv_scores_wine.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {cv_scores_wine.mean():.3f}')\n",
    "ax.fill_between(folds,\n",
    "                cv_scores_wine.mean() - cv_scores_wine.std(),\n",
    "                cv_scores_wine.mean() + cv_scores_wine.std(),\n",
    "                alpha=0.2, color='red', label='¬±1 Std Dev')\n",
    "ax.set_xlabel('Fold Number', fontsize=11)\n",
    "ax.set_ylabel('Accuracy', fontsize=11)\n",
    "ax.set_title('üç∑ Wine: Cross-Validation Scores', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(folds)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Housing CV scores (RMSE)\n",
    "ax = axes[2]\n",
    "ax.plot(folds, cv_scores_housing_rmse, 'o-', linewidth=2, markersize=8, color='green', label='Fold RMSE')\n",
    "ax.axhline(cv_scores_housing_rmse.mean(), color='red', linestyle='--', linewidth=2, \n",
    "          label=f'Mean: ${cv_scores_housing_rmse.mean():.2f}')\n",
    "ax.fill_between(folds,\n",
    "                cv_scores_housing_rmse.mean() - cv_scores_housing_rmse.std(),\n",
    "                cv_scores_housing_rmse.mean() + cv_scores_housing_rmse.std(),\n",
    "                alpha=0.2, color='red', label='¬±1 Std Dev')\n",
    "ax.set_xlabel('Fold Number', fontsize=11)\n",
    "ax.set_ylabel('RMSE', fontsize=11)\n",
    "ax.set_title('üè† Housing: Cross-Validation RMSE', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(folds)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Cross-validation provides robust performance estimates!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b3f9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **8. Hyperparameter Tuning - Finding the Best Settings**\n",
    "\n",
    "**What are Hyperparameters?**\n",
    "- Settings you choose BEFORE training (not learned from data)\n",
    "- Examples: number of trees in Random Forest, learning rate, max depth\n",
    "- Different values ‚Üí different model performance\n",
    "\n",
    "**Grid Search:**\n",
    "- Try all combinations of hyperparameter values\n",
    "- Use cross-validation to evaluate each combination\n",
    "- Select the best performing combination\n",
    "\n",
    "**Example:**\n",
    "- If testing 3 values for `n_estimators` and 3 for `max_depth`\n",
    "- Grid Search tries all 3 √ó 3 = 9 combinations\n",
    "- With 5-fold CV, that's 9 √ó 5 = 45 model trainings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522468d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"HYPERPARAMETER TUNING WITH GRID SEARCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: Grid Search on Iris Random Forest\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ TUNING RANDOM FOREST ON IRIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Define hyperparameter grid to search\n",
    "# We'll test different combinations of these values\n",
    "param_grid_iris = {\n",
    "    'n_estimators': [50, 100, 200],        # Number of trees\n",
    "    'max_depth': [3, 5, 10, None],         # Maximum tree depth\n",
    "    'min_samples_split': [2, 5, 10]        # Minimum samples to split node\n",
    "}\n",
    "\n",
    "# Calculate total combinations\n",
    "total_combinations = (len(param_grid_iris['n_estimators']) * \n",
    "                     len(param_grid_iris['max_depth']) * \n",
    "                     len(param_grid_iris['min_samples_split']))\n",
    "print(f\"Testing {total_combinations} hyperparameter combinations with 5-fold CV\")\n",
    "print(f\"Total model trainings: {total_combinations * 5}\")\n",
    "\n",
    "# Create base model\n",
    "rf_base = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Create GridSearchCV object\n",
    "# This will try all combinations and find the best\n",
    "grid_search_iris = GridSearchCV(\n",
    "    estimator=rf_base,      # Model to tune\n",
    "    param_grid=param_grid_iris,  # Hyperparameters to try\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='accuracy',     # Metric to optimize\n",
    "    n_jobs=-1,              # Use all CPU cores\n",
    "    verbose=1               # Show progress\n",
    ")\n",
    "\n",
    "# Run grid search\n",
    "print(\"\\nüîç Searching for best hyperparameters...\")\n",
    "grid_search_iris.fit(X_iris_scaled, y_iris)\n",
    "\n",
    "# Best parameters found\n",
    "print(f\"\\n‚úÖ Grid Search Complete!\")\n",
    "print(f\"\\nüèÜ Best Hyperparameters:\")\n",
    "for param, value in grid_search_iris.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Best Cross-Validation Score: {grid_search_iris.best_score_:.4f}\")\n",
    "\n",
    "# Compare with default parameters\n",
    "rf_default = RandomForestClassifier(random_state=42)\n",
    "default_scores = cross_val_score(rf_default, X_iris_scaled, y_iris, cv=5)\n",
    "print(f\"üìä Default Parameters Score:    {default_scores.mean():.4f}\")\n",
    "print(f\"\\n‚ú® Improvement: {(grid_search_iris.best_score_ - default_scores.mean()):.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: Grid Search on Wine Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2Ô∏è‚É£ TUNING RANDOM FOREST ON WINE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Smaller grid for Wine (fewer samples)\n",
    "param_grid_wine = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "print(f\"Testing {3 * 3 * 2} combinations...\")\n",
    "\n",
    "grid_search_wine = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid_wine,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"üîç Searching...\")\n",
    "grid_search_wine.fit(X_wine_scaled, y_wine)\n",
    "\n",
    "print(f\"\\nüèÜ Best Parameters: {grid_search_wine.best_params_}\")\n",
    "print(f\"üìä Best CV Score: {grid_search_wine.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: Grid Search on Housing Dataset (Regression)\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3Ô∏è‚É£ TUNING RANDOM FOREST ON HOUSING\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Grid for regression\n",
    "param_grid_housing = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "print(f\"Testing {2 * 3 * 2} combinations...\")\n",
    "\n",
    "grid_search_housing = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    param_grid_housing,\n",
    "    cv=3,  # Use 3 folds (dataset is large)\n",
    "    scoring='r2',  # Optimize R¬≤ score\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"üîç Searching (this may take a minute with 16k+ samples)...\")\n",
    "grid_search_housing.fit(X_housing_scaled, y_housing)\n",
    "\n",
    "print(f\"\\nüèÜ Best Parameters: {grid_search_housing.best_params_}\")\n",
    "print(f\"üìä Best CV R¬≤ Score: {grid_search_housing.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: Visualizing Grid Search Results\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4Ô∏è‚É£ ANALYZING GRID SEARCH RESULTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Extract results for Iris\n",
    "cv_results_iris = grid_search_iris.cv_results_\n",
    "\n",
    "# Get top 10 parameter combinations\n",
    "top_10_indices = np.argsort(cv_results_iris['mean_test_score'])[-10:][::-1]\n",
    "\n",
    "print(\"\\nüèÜ Top 10 Hyperparameter Combinations for Iris:\")\n",
    "print(\"-\" * 70)\n",
    "for rank, idx in enumerate(top_10_indices, 1):\n",
    "    params = cv_results_iris['params'][idx]\n",
    "    score = cv_results_iris['mean_test_score'][idx]\n",
    "    print(f\"{rank:2d}. Score: {score:.4f} | n_est={params['n_estimators']:3d}, \"\n",
    "          f\"depth={str(params['max_depth']):4s}, split={params['min_samples_split']:2d}\")\n",
    "\n",
    "# Visualize effect of n_estimators\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"5Ô∏è‚É£ VISUALIZING HYPERPARAMETER IMPACT\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Extract scores for different n_estimators (fixing other params)\n",
    "ax = axes[0]\n",
    "n_est_values = sorted(set([params['n_estimators'] for params in cv_results_iris['params']]))\n",
    "mean_scores_by_n_est = []\n",
    "\n",
    "for n_est in n_est_values:\n",
    "    # Get all scores with this n_estimators value\n",
    "    scores = [cv_results_iris['mean_test_score'][i] \n",
    "              for i, params in enumerate(cv_results_iris['params']) \n",
    "              if params['n_estimators'] == n_est]\n",
    "    mean_scores_by_n_est.append(np.mean(scores))\n",
    "\n",
    "ax.plot(n_est_values, mean_scores_by_n_est, 'o-', linewidth=2, markersize=10, color='blue')\n",
    "ax.set_xlabel('Number of Trees (n_estimators)', fontsize=11)\n",
    "ax.set_ylabel('Mean CV Accuracy', fontsize=11)\n",
    "ax.set_title('Effect of Number of Trees', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Compare all three datasets\n",
    "ax = axes[1]\n",
    "models = ['Iris', 'Wine', 'Housing']\n",
    "best_scores = [grid_search_iris.best_score_, \n",
    "               grid_search_wine.best_score_, \n",
    "               grid_search_housing.best_score_]\n",
    "colors = ['#3498db', '#9b59b6', '#2ecc71']\n",
    "bars = ax.bar(models, best_scores, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Best CV Score', fontsize=12)\n",
    "ax.set_title('Best Tuned Model Performance', fontsize=12, fontweight='bold')\n",
    "ax.set_ylim(0.8, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, score in zip(bars, best_scores):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "            f'{score:.4f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Hyperparameter tuning complete!\")\n",
    "print(\"üéØ Grid Search found optimal settings for each dataset\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186ffe30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **9. ML Pipelines - Professional Workflow**\n",
    "\n",
    "**What is a Pipeline?**\n",
    "- Chains together preprocessing steps + model into one object\n",
    "- Ensures preprocessing is done consistently on train and test data\n",
    "- Prevents data leakage automatically\n",
    "- Makes code cleaner and more maintainable\n",
    "\n",
    "**Why Pipelines Matter:**\n",
    "1. **Prevents Data Leakage** - Scaler fits only on training fold in CV\n",
    "2. **Cleaner Code** - One `.fit()` instead of multiple steps\n",
    "3. **Easier Deployment** - Save entire pipeline as single object\n",
    "4. **Grid Search Integration** - Can tune preprocessing AND model together\n",
    "\n",
    "**Pipeline Structure:**\n",
    "```\n",
    "Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Step 1: Scale features\n",
    "    ('classifier', RandomForest())     # Step 2: Train model\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b888cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MACHINE LEARNING PIPELINES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: Creating a Simple Pipeline for Iris\n",
    "# ============================================================================\n",
    "print(\"\\n1Ô∏è‚É£ BUILDING A PIPELINE FOR IRIS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create pipeline: Scaler ‚Üí Classifier\n",
    "# Pipeline automatically applies steps in sequence\n",
    "pipeline_iris = Pipeline([\n",
    "    ('scaler', StandardScaler()),           # Step 1: Normalize features\n",
    "    ('classifier', LogisticRegression(max_iter=200, random_state=42))  # Step 2: Train model\n",
    "])\n",
    "\n",
    "print(\"Pipeline created with 2 steps:\")\n",
    "print(\"  1. StandardScaler (normalize features)\")\n",
    "print(\"  2. LogisticRegression (classify)\")\n",
    "\n",
    "# Train pipeline with single fit() call\n",
    "# Internally: fits scaler on train data, transforms train data, trains classifier\n",
    "print(\"\\nTraining pipeline on Iris...\")\n",
    "pipeline_iris.fit(X_train_iris, y_train_iris)\n",
    "\n",
    "# Make predictions\n",
    "# Internally: transforms test data with fitted scaler, predicts with classifier\n",
    "y_pred_pipeline = pipeline_iris.predict(X_test_iris)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_pipeline = accuracy_score(y_test_iris, y_pred_pipeline)\n",
    "print(f\"\\n‚úÖ Pipeline Accuracy: {accuracy_pipeline:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: Pipeline with Cross-Validation\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"2Ô∏è‚É£ PIPELINE WITH CROSS-VALIDATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Why pipelines are crucial for CV:\n",
    "# Without pipeline: Scaler sees ALL data ‚Üí DATA LEAKAGE\n",
    "# With pipeline: Each CV fold fits scaler only on that fold's training data\n",
    "\n",
    "print(\"Running 5-fold CV with pipeline (prevents data leakage)...\")\n",
    "\n",
    "# Cross-validate the entire pipeline\n",
    "cv_scores_pipeline = cross_val_score(\n",
    "    pipeline_iris,      # Entire pipeline\n",
    "    X_iris,             # RAW features (not scaled)\n",
    "    y_iris,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Cross-Validation Scores: {cv_scores_pipeline}\")\n",
    "print(f\"üìà Mean: {cv_scores_pipeline.mean():.4f} ¬± {cv_scores_pipeline.std():.4f}\")\n",
    "\n",
    "print(\"\\nüí° Pipeline automatically:\")\n",
    "print(\"   ‚Ä¢ Fits scaler on training folds only\")\n",
    "print(\"   ‚Ä¢ Transforms test folds with training statistics\")\n",
    "print(\"   ‚Ä¢ Prevents data leakage in every CV fold\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: Pipeline with Grid Search\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"3Ô∏è‚É£ PIPELINE + GRID SEARCH = ULTIMATE COMBO\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create pipeline with Random Forest\n",
    "pipeline_rf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Define parameter grid\n",
    "# Use 'stepname__parameter' syntax to specify parameters\n",
    "param_grid_pipeline = {\n",
    "    'classifier__n_estimators': [50, 100, 150],     # RF parameter\n",
    "    'classifier__max_depth': [5, 10, None],         # RF parameter\n",
    "    'classifier__min_samples_split': [2, 5]         # RF parameter\n",
    "}\n",
    "\n",
    "print(f\"Testing {3 * 3 * 2} hyperparameter combinations...\")\n",
    "\n",
    "# Grid search on pipeline\n",
    "grid_search_pipeline = GridSearchCV(\n",
    "    pipeline_rf,\n",
    "    param_grid_pipeline,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nüîç Running Grid Search with Pipeline on Iris...\")\n",
    "grid_search_pipeline.fit(X_iris, y_iris)  # RAW data (pipeline handles scaling)\n",
    "\n",
    "print(f\"\\nüèÜ Best Parameters:\")\n",
    "for param, value in grid_search_pipeline.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "print(f\"\\nüìä Best Score: {grid_search_pipeline.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: Complete Pipeline for Wine Dataset\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"4Ô∏è‚É£ COMPLETE PIPELINE FOR WINE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Build complete ML workflow\n",
    "pipeline_wine = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train pipeline\n",
    "print(\"Training Wine classification pipeline...\")\n",
    "pipeline_wine.fit(X_train_wine, y_train_wine)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_wine_pipeline = pipeline_wine.predict(X_test_wine)\n",
    "accuracy_wine_pipeline = accuracy_score(y_test_wine, y_pred_wine_pipeline)\n",
    "\n",
    "print(f\"‚úÖ Wine Pipeline Accuracy: {accuracy_wine_pipeline:.4f}\")\n",
    "\n",
    "# Show confusion matrix\n",
    "cm_wine_pipeline = confusion_matrix(y_test_wine, y_pred_wine_pipeline)\n",
    "print(\"\\nüìã Confusion Matrix:\")\n",
    "print(cm_wine_pipeline)\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: Regression Pipeline for Housing\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"5Ô∏è‚É£ REGRESSION PIPELINE FOR HOUSING\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create regression pipeline\n",
    "pipeline_housing = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "# Train on subset for speed (first 5000 samples)\n",
    "print(\"Training Housing regression pipeline...\")\n",
    "X_train_subset = X_train_housing[:5000]\n",
    "y_train_subset = y_train_housing[:5000]\n",
    "\n",
    "pipeline_housing.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_housing_pipeline = pipeline_housing.predict(X_test_housing)\n",
    "mae_housing_pipeline = mean_absolute_error(y_test_housing, y_pred_housing_pipeline)\n",
    "r2_housing_pipeline = r2_score(y_test_housing, y_pred_housing_pipeline)\n",
    "\n",
    "print(f\"\\n‚úÖ Housing Pipeline Performance:\")\n",
    "print(f\"   MAE:  ${mae_housing_pipeline:.2f}\")\n",
    "print(f\"   R¬≤:   {r2_housing_pipeline:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: Pipeline Benefits Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"6Ô∏è‚É£ WHY USE PIPELINES? COMPARISON\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"\\n‚ùå WITHOUT Pipeline (manual workflow):\")\n",
    "print(\"\"\"\n",
    "   # Step 1: Scale training data\n",
    "   scaler = StandardScaler()\n",
    "   X_train_scaled = scaler.fit_transform(X_train)\n",
    "   \n",
    "   # Step 2: Scale test data (RISK: might use test data accidentally)\n",
    "   X_test_scaled = scaler.transform(X_test)\n",
    "   \n",
    "   # Step 3: Train model\n",
    "   model = RandomForest()\n",
    "   model.fit(X_train_scaled, y_train)\n",
    "   \n",
    "   # Step 4: Predict\n",
    "   y_pred = model.predict(X_test_scaled)\n",
    "   \n",
    "   # Problem: Easy to make mistakes, especially in CV!\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ WITH Pipeline (professional workflow):\")\n",
    "print(\"\"\"\n",
    "   # Create pipeline\n",
    "   pipeline = Pipeline([\n",
    "       ('scaler', StandardScaler()),\n",
    "       ('model', RandomForest())\n",
    "   ])\n",
    "   \n",
    "   # Single fit handles everything correctly\n",
    "   pipeline.fit(X_train, y_train)\n",
    "   \n",
    "   # Single predict handles everything correctly\n",
    "   y_pred = pipeline.predict(X_test)\n",
    "   \n",
    "   # Benefits: Clean, safe, no data leakage possible!\n",
    "\"\"\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 7: Visualizing Pipeline Performance\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"7Ô∏è‚É£ PIPELINE PERFORMANCE VISUALIZATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Classification pipelines comparison\n",
    "ax = axes[0]\n",
    "datasets = ['Iris', 'Wine']\n",
    "accuracies = [accuracy_pipeline, accuracy_wine_pipeline]\n",
    "colors = ['#3498db', '#9b59b6']\n",
    "bars = ax.bar(datasets, accuracies, color=colors, alpha=0.7, edgecolor='black', width=0.5)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "ax.set_title('Pipeline Performance - Classification', fontsize=13, fontweight='bold')\n",
    "ax.set_ylim(0.9, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "            f'{height:.2%}', ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Regression pipeline\n",
    "ax = axes[1]\n",
    "metrics = ['MAE', 'R¬≤ Score']\n",
    "values = [mae_housing_pipeline, r2_housing_pipeline]\n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "bars = ax.bar(metrics, values, color=colors, alpha=0.7, edgecolor='black', width=0.5)\n",
    "ax.set_ylabel('Value', fontsize=12)\n",
    "ax.set_title('Pipeline Performance - Regression (Housing)', fontsize=13, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    if 'MAE' in bar.get_x():\n",
    "        label = f'${val:.2f}'\n",
    "    else:\n",
    "        label = f'{val:.4f}'\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "            label, ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Pipelines provide clean, safe, professional ML workflows!\")\n",
    "print(\"üéØ Always use pipelines in production code\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bad1892",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **10. Putting It All Together - Complete ML Workflow**\n",
    "\n",
    "**The Professional ML Process:**\n",
    "\n",
    "1. **Load & Explore Data** üìä\n",
    "   - Understand features, target, distributions\n",
    "   \n",
    "2. **Train/Test Split** ‚úÇÔ∏è\n",
    "   - Separate data for training and evaluation\n",
    "   \n",
    "3. **Build Pipeline** üîß\n",
    "   - Preprocessing + Model in one object\n",
    "   \n",
    "4. **Cross-Validation** üîÑ\n",
    "   - Robust performance estimate\n",
    "   \n",
    "5. **Hyperparameter Tuning** üéØ\n",
    "   - Find optimal settings with Grid Search\n",
    "   \n",
    "6. **Final Evaluation** ‚úÖ\n",
    "   - Test on held-out test set\n",
    "   \n",
    "7. **Analyze Results** üìà\n",
    "   - Confusion matrix, feature importance, visualizations\n",
    "\n",
    "Let's demonstrate the complete workflow on a new problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COMPLETE ML WORKFLOW - IRIS SPECIES CLASSIFIER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: Load and Explore Data\n",
    "# ============================================================================\n",
    "print(\"\\nüìä STEP 1: LOAD AND EXPLORE DATA\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Load Iris dataset (we already have it, but let's start fresh)\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X_full, y_full = iris.data, iris.target\n",
    "\n",
    "print(f\"Dataset: {len(X_full)} samples, {X_full.shape[1]} features\")\n",
    "print(f\"Classes: {iris.target_names}\")\n",
    "print(f\"Class distribution: {np.bincount(y_full)}\")\n",
    "print(f\"\\nFeature names: {iris.feature_names}\")\n",
    "print(f\"Feature ranges:\")\n",
    "for i, name in enumerate(iris.feature_names):\n",
    "    print(f\"  {name:20s}: {X_full[:, i].min():.2f} to {X_full[:, i].max():.2f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: Train/Test Split\n",
    "# ============================================================================\n",
    "print(\"\\n\\n‚úÇÔ∏è STEP 2: TRAIN/TEST SPLIT\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42, stratify=y_full\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train_full)} samples\")\n",
    "print(f\"Test set:     {len(X_test_full)} samples\")\n",
    "print(f\"Test set is HELD OUT until final evaluation!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: Build Pipeline\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüîß STEP 3: BUILD PIPELINE\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Create pipeline with preprocessing + model\n",
    "final_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "print(\"Pipeline created:\")\n",
    "print(\"  Step 1: StandardScaler\")\n",
    "print(\"  Step 2: RandomForestClassifier\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: Cross-Validation on Training Set\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüîÑ STEP 4: CROSS-VALIDATION (Training Set Only)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"Running 5-fold CV on training set...\")\n",
    "cv_scores = cross_val_score(\n",
    "    final_pipeline, \n",
    "    X_train_full, \n",
    "    y_train_full, \n",
    "    cv=5, \n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "print(f\"\\nCV Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "print(\"‚úÖ Model looks good on training data!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: Hyperparameter Tuning\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüéØ STEP 5: HYPERPARAMETER TUNING (Training Set Only)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid_final = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [3, 5, 10],\n",
    "    'classifier__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "print(f\"Testing {3 * 3 * 2} combinations with 5-fold CV...\")\n",
    "\n",
    "# Grid search\n",
    "grid_search_final = GridSearchCV(\n",
    "    final_pipeline,\n",
    "    param_grid_final,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"üîç Searching for best hyperparameters...\")\n",
    "grid_search_final.fit(X_train_full, y_train_full)\n",
    "\n",
    "print(f\"\\nüèÜ Best Parameters Found:\")\n",
    "for param, value in grid_search_final.best_params_.items():\n",
    "    print(f\"   {param}: {value}\")\n",
    "print(f\"\\nüìä Best CV Score: {grid_search_final.best_score_:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: Final Evaluation on Test Set\n",
    "# ============================================================================\n",
    "print(\"\\n\\n‚úÖ STEP 6: FINAL EVALUATION ON TEST SET\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Get best model from grid search\n",
    "best_model = grid_search_final.best_estimator_\n",
    "\n",
    "# NOW we use the test set for the first time\n",
    "y_pred_final = best_model.predict(X_test_full)\n",
    "\n",
    "# Calculate final metrics\n",
    "final_accuracy = accuracy_score(y_test_full, y_pred_final)\n",
    "final_cm = confusion_matrix(y_test_full, y_pred_final)\n",
    "\n",
    "print(f\"üéâ FINAL TEST ACCURACY: {final_accuracy:.2%}\")\n",
    "print(f\"\\nüìã Confusion Matrix:\")\n",
    "print(final_cm)\n",
    "print(f\"\\nüìà Classification Report:\")\n",
    "print(classification_report(y_test_full, y_pred_final, target_names=iris.target_names))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: Analyze Results\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüìà STEP 7: ANALYZE RESULTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Feature importance\n",
    "best_rf = best_model.named_steps['classifier']\n",
    "feature_importance = best_rf.feature_importances_\n",
    "\n",
    "print(\"üéØ Feature Importance:\")\n",
    "importance_pairs = sorted(zip(iris.feature_names, feature_importance), \n",
    "                         key=lambda x: x[1], reverse=True)\n",
    "for name, importance in importance_pairs:\n",
    "    bar = '‚ñà' * int(importance * 50)\n",
    "    print(f\"  {name:20s}: {bar} {importance:.3f}\")\n",
    "\n",
    "print(\"\\nüí° Insights:\")\n",
    "print(f\"  ‚Ä¢ Most important feature: {importance_pairs[0][0]}\")\n",
    "print(f\"  ‚Ä¢ Least important feature: {importance_pairs[-1][0]}\")\n",
    "print(f\"  ‚Ä¢ Model correctly classified {int(final_accuracy * len(y_test_full))}/{len(y_test_full)} test samples\")\n",
    "\n",
    "# Misclassified samples\n",
    "misclassified = np.where(y_pred_final != y_test_full)[0]\n",
    "if len(misclassified) > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Misclassified Samples: {len(misclassified)}\")\n",
    "    for idx in misclassified[:3]:  # Show first 3\n",
    "        print(f\"  Sample {idx}: Predicted {iris.target_names[y_pred_final[idx]]}, \"\n",
    "              f\"Actually {iris.target_names[y_test_full[idx]]}\")\n",
    "else:\n",
    "    print(\"\\nüéâ PERFECT! All test samples classified correctly!\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: Comprehensive Visualization\n",
    "# ============================================================================\n",
    "print(\"\\n\\nüé® STEP 8: COMPREHENSIVE VISUALIZATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Confusion Matrix Heatmap\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "im = ax1.imshow(final_cm, cmap='Blues', aspect='auto')\n",
    "ax1.set_xticks(range(3))\n",
    "ax1.set_yticks(range(3))\n",
    "ax1.set_xticklabels(iris.target_names, rotation=45)\n",
    "ax1.set_yticklabels(iris.target_names)\n",
    "ax1.set_xlabel('Predicted')\n",
    "ax1.set_ylabel('Actual')\n",
    "ax1.set_title('Confusion Matrix', fontweight='bold')\n",
    "# Add text annotations\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        text = ax1.text(j, i, final_cm[i, j], ha='center', va='center', \n",
    "                       color='white' if final_cm[i, j] > final_cm.max()/2 else 'black',\n",
    "                       fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax1)\n",
    "\n",
    "# 2. Feature Importance Bar Chart\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "features = [name.split()[0] for name in iris.feature_names]  # Shorten names\n",
    "importance_sorted = sorted(feature_importance, reverse=True)\n",
    "colors_sorted = plt.cm.viridis(np.linspace(0, 1, 4))\n",
    "bars = ax2.barh(range(4), importance_sorted, color=colors_sorted, edgecolor='black')\n",
    "ax2.set_yticks(range(4))\n",
    "ax2.set_yticklabels([name for name, _ in importance_pairs])\n",
    "ax2.set_xlabel('Importance')\n",
    "ax2.set_title('Feature Importance', fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "for i, (bar, val) in enumerate(zip(bars, importance_sorted)):\n",
    "    ax2.text(val + 0.01, i, f'{val:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# 3. CV Scores Distribution\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "cv_results = grid_search_final.cv_results_\n",
    "top_10_scores = sorted(cv_results['mean_test_score'])[-10:]\n",
    "ax3.hist(top_10_scores, bins=10, color='green', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(grid_search_final.best_score_, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Best: {grid_search_final.best_score_:.4f}')\n",
    "ax3.set_xlabel('CV Accuracy')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Top 10 Model Scores', fontweight='bold')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Petal Length vs Width (colored by prediction)\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "scatter = ax4.scatter(X_test_full[:, 2], X_test_full[:, 3], \n",
    "                     c=y_pred_final, cmap='viridis', s=100, alpha=0.6, edgecolor='black')\n",
    "ax4.set_xlabel('Petal Length')\n",
    "ax4.set_ylabel('Petal Width')\n",
    "ax4.set_title('Predictions: Petal Features', fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax4, ticks=[0, 1, 2], \n",
    "            label='Predicted Class')\n",
    "\n",
    "# 5. Sepal Length vs Width (colored by actual)\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "scatter = ax5.scatter(X_test_full[:, 0], X_test_full[:, 1], \n",
    "                     c=y_test_full, cmap='viridis', s=100, alpha=0.6, edgecolor='black')\n",
    "ax5.set_xlabel('Sepal Length')\n",
    "ax5.set_ylabel('Sepal Width')\n",
    "ax5.set_title('Actual: Sepal Features', fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax5, ticks=[0, 1, 2], label='Actual Class')\n",
    "\n",
    "# 6. Model Comparison\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "stages = ['CV on Train', 'Final Test']\n",
    "scores = [grid_search_final.best_score_, final_accuracy]\n",
    "bars = ax6.bar(stages, scores, color=['blue', 'green'], alpha=0.7, edgecolor='black')\n",
    "ax6.set_ylabel('Accuracy')\n",
    "ax6.set_title('Train vs Test Performance', fontweight='bold')\n",
    "ax6.set_ylim(0.9, 1.0)\n",
    "ax6.grid(axis='y', alpha=0.3)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "            f'{height:.2%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 7. Learning Curve (CV folds)\n",
    "ax7 = fig.add_subplot(gs[2, :])\n",
    "fold_scores = cv_scores\n",
    "folds = list(range(1, 6))\n",
    "ax7.plot(folds, fold_scores, 'o-', linewidth=2, markersize=10, color='blue', label='Fold Accuracy')\n",
    "ax7.axhline(fold_scores.mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {fold_scores.mean():.4f}')\n",
    "ax7.fill_between(folds, fold_scores.mean() - fold_scores.std(), \n",
    "                fold_scores.mean() + fold_scores.std(),\n",
    "                alpha=0.2, color='red', label='¬±1 Std Dev')\n",
    "ax7.set_xlabel('Fold Number', fontsize=12)\n",
    "ax7.set_ylabel('Accuracy', fontsize=12)\n",
    "ax7.set_title('Cross-Validation Consistency', fontsize=13, fontweight='bold')\n",
    "ax7.set_xticks(folds)\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('üéâ Complete ML Workflow: Iris Classification üéâ', \n",
    "            fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéì COMPLETE ML WORKFLOW SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\"\"\n",
    "‚úÖ Step 1: Loaded {len(X_full)} samples with {X_full.shape[1]} features\n",
    "‚úÖ Step 2: Split into {len(X_train_full)} train, {len(X_test_full)} test samples\n",
    "‚úÖ Step 3: Built pipeline with StandardScaler + RandomForest\n",
    "‚úÖ Step 4: Cross-validated on training set ‚Üí {cv_scores.mean():.4f} accuracy\n",
    "‚úÖ Step 5: Tuned hyperparameters ‚Üí {grid_search_final.best_score_:.4f} CV accuracy\n",
    "‚úÖ Step 6: Evaluated on test set ‚Üí {final_accuracy:.2%} accuracy\n",
    "‚úÖ Step 7: Analyzed feature importance and misclassifications\n",
    "‚úÖ Step 8: Created comprehensive visualizations\n",
    "\n",
    "üèÜ FINAL MODEL PERFORMANCE:\n",
    "   ‚Ä¢ Test Accuracy: {final_accuracy:.2%}\n",
    "   ‚Ä¢ Correctly classified: {int(final_accuracy * len(y_test_full))}/{len(y_test_full)} samples\n",
    "   ‚Ä¢ Best hyperparameters: {grid_search_final.best_params_}\n",
    "   \n",
    "üéØ KEY TAKEAWAYS:\n",
    "   1. Always split data BEFORE any analysis\n",
    "   2. Use pipelines to prevent data leakage\n",
    "   3. Tune hyperparameters with cross-validation\n",
    "   4. Evaluate on test set ONLY ONCE at the end\n",
    "   5. Analyze results to understand model behavior\n",
    "   \n",
    "üí° THIS IS THE PROFESSIONAL ML WORKFLOW!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"‚ú® Chapter 04: scikit-learn Machine Learning - COMPLETE! ‚ú®\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b12c0eb",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"projects\"></a>\n",
    "## üöÄ Chapter Completed! Now Practice with Projects\n",
    "\n",
    "### üéâ Congratulations! You've Mastered scikit-learn Basics!\n",
    "\n",
    "### üìù Recommended Projects (in order):\n",
    "\n",
    "#### 1. **Project 03: Classification** ‚≠ê‚≠ê‚≠ê Intermediate\n",
    "**Link:** [Open Project 03](../projects/Project_03_Classification.md)\n",
    "\n",
    "**Time:** 4-5 hours\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. **Project 04: Regression** ‚≠ê‚≠ê‚≠ê Intermediate\n",
    "**Link:** [Open Project 04](../projects/Project_04_Regression.md)\n",
    "\n",
    "**Time:** 3-4 hours\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. **Project 05: Clustering & PCA** ‚≠ê‚≠ê Intermediate\n",
    "**Link:** [Open Project 05](../projects/Project_05_Clustering_PCA.md)\n",
    "\n",
    "**Time:** 3 hours\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. **Project 06: End-to-End ML Pipeline** ‚≠ê‚≠ê‚≠ê‚≠ê Advanced\n",
    "**Link:** [Open Project 06](../projects/Project_06_EndToEnd.md)\n",
    "\n",
    "**Time:** 6-8 hours\n",
    "\n",
    "---\n",
    "\n",
    "## üéì You're Now Ready for Real ML Projects!\n",
    "\n",
    "### üí° Next Steps:\n",
    "1. Complete Projects 03-06 to build your ML portfolio\n",
    "2. Participate in Kaggle competitions\n",
    "3. Build your own ML projects with real data\n",
    "4. Learn advanced topics: Deep Learning, NLP, Computer Vision\n",
    "\n",
    "---\n",
    "\n",
    "## üîó Navigation\n",
    "\n",
    "- **Previous**: [Chapter 03: Matplotlib](03_Matplotlib_Visualization.ipynb)\n",
    "- **Home**: [START HERE](../START_HERE.md)\n",
    "- **Index**: [Main Index](../index.md)\n",
    "- **All Projects**: [Projects Overview](../projects/README.md)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
